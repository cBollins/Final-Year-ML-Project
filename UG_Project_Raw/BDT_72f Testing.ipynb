{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f39aa7e4-f1ff-4155-83a3-6159221298b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: uproot in /opt/conda/lib/python3.11/site-packages (5.5.2)\n",
      "Requirement already satisfied: awkward in /opt/conda/lib/python3.11/site-packages (2.7.4)\n",
      "Requirement already satisfied: cramjam>=2.5.0 in /opt/conda/lib/python3.11/site-packages (from uproot) (2.9.1)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from uproot) (2024.10.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from uproot) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from uproot) (24.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from uproot) (3.5.0)\n",
      "Requirement already satisfied: awkward-cpp==44 in /opt/conda/lib/python3.11/site-packages (from awkward) (44)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /opt/conda/lib/python3.11/site-packages (from awkward) (7.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata>=4.13.0->awkward) (3.17.0)\n",
      "Requirement already satisfied: xgboost in /opt/conda/lib/python3.11/site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /opt/conda/lib/python3.11/site-packages (from xgboost) (2.25.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from xgboost) (1.14.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install uproot awkward \n",
    "!pip install xgboost\n",
    "from uproot_io import Events, View\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.spatial import ConvexHull\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "cheated_0 = Events('CheatedRecoFile_0.root') # training set\n",
    "cheated_5 = Events(\"CheatedRecoFile_5.root\") # testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6f377d5-a6e4-47fc-868e-28fe75c0f13f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" BDT Feature Functions  \"\"\" \n",
    "\n",
    "# track/shower\n",
    "def correlation(events, event_idx):\n",
    "    x_hits = events.reco_hits_x_w[event_idx]\n",
    "    w_hits = events.reco_hits_w[event_idx] \n",
    "\n",
    "    # Check if there are valid hits\n",
    "    if len(w_hits) == len(x_hits) and len(w_hits) > 15: # talk about advantages and disadvantages of results with a threshold \n",
    "        if np.std(x_hits) == 0 or np.std(w_hits) == 0:\n",
    "            return None  # No valid correlation if there's no variation in data\n",
    "        \n",
    "        correlation = np.corrcoef(x_hits, w_hits)[0, 1]\n",
    "        \n",
    "        # Fit line using w_hits for x and calculate predicted y-values\n",
    "        line_fit = np.polyfit(w_hits, x_hits, 1)\n",
    "        line_y_pred = np.polyval(line_fit, w_hits)\n",
    "        \n",
    "        # Calculate line error between predicted and actual x_hits\n",
    "        line_error = np.mean((x_hits - line_y_pred) ** 2)\n",
    "        \n",
    "        # Normalize scores\n",
    "        correlation_score = abs(correlation) if not np.isnan(correlation) else 0\n",
    "        error_score = max(0, 1 - line_error / 20) if line_error < 20 else 0\n",
    "        \n",
    "        # Weighted score\n",
    "        line_score = (correlation_score * 0.7) + (error_score * 0.3)\n",
    "        \n",
    "        return (line_score * 100)  # Return the score and category\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def noise(events, event_idx, eps=2, min_samples=5):\n",
    "    # Extract hit positions (no PDG filtering, just use reco hits)\n",
    "    x_hits = events.reco_hits_x_w[event_idx]\n",
    "    w_hits = events.reco_hits_w[event_idx]\n",
    "\n",
    "    # Check if there are valid hits\n",
    "    if len(w_hits) == len(x_hits) and len(w_hits) > 15:\n",
    "        # Combine the coordinates for clustering\n",
    "        hits_coordinates = np.column_stack((w_hits, x_hits))\n",
    "\n",
    "        # Apply DBSCAN clustering\n",
    "        db = DBSCAN(eps=eps, min_samples=min_samples).fit(hits_coordinates)\n",
    "        labels = db.labels_\n",
    "\n",
    "        # Count noise points (labeled as -1)\n",
    "        n_noise = np.sum(labels == -1)\n",
    "\n",
    "        # Count clusters (unique labels excluding -1)\n",
    "        unique_clusters = set(labels) - {-1}\n",
    "        n_clusters = len(unique_clusters)\n",
    "\n",
    "        return n_noise + n_clusters\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "def rms(events, event_idx):\n",
    "    w_hits = events.reco_hits_w[event_idx]\n",
    "    x_hits = events.reco_hits_x_w[event_idx]\n",
    "\n",
    "    if len(w_hits) == len(x_hits) and len(w_hits) > 15:\n",
    "        slope, intercept = np.polyfit(w_hits, x_hits, 1)\n",
    "        \n",
    "        actual = x_hits\n",
    "        predicted = slope * w_hits + intercept\n",
    "        \n",
    "        meanSquaredError = ((predicted - actual) ** 2).mean()\n",
    "        return np.sqrt(meanSquaredError)\n",
    "    else: \n",
    "        return None \n",
    "        \n",
    "def angle(events, event_idx):\n",
    "    x_hits = events.reco_hits_x_w[event_idx]\n",
    "    w_hits = events.reco_hits_w[event_idx]\n",
    "\n",
    "    if len(w_hits) == len(x_hits) and len(w_hits) > 15:\n",
    "        # Fit the best-fit line\n",
    "        line_fit = np.polyfit(w_hits, x_hits, 1)\n",
    "        line_slope = line_fit[0]\n",
    "        line_intercept = line_fit[1]\n",
    "\n",
    "        # Calculate residuals (distance from the line)\n",
    "        line_y_pred = np.polyval(line_fit, w_hits)\n",
    "        residuals = np.abs(x_hits - line_y_pred)\n",
    "\n",
    "        # Find the index of the furthest point\n",
    "        furthest_idx = np.argmax(residuals)\n",
    "        furthest_point = np.array([x_hits[furthest_idx], w_hits[furthest_idx]])\n",
    "\n",
    "        # Start of the line is at the minimum W-coordinate\n",
    "        min_w = np.min(w_hits)\n",
    "        start_point = np.array([line_slope * min_w + line_intercept, min_w])\n",
    "\n",
    "        # End of the red line (best-fit line) at the maximum W-coordinate\n",
    "        max_w = np.max(w_hits)\n",
    "        end_of_red_line = np.array([line_slope * max_w + line_intercept, max_w])\n",
    "\n",
    "        # Calculate the lengths of the three sides of the triangle\n",
    "        red_line_length = np.linalg.norm(end_of_red_line - start_point)  # Distance between start and end of red line\n",
    "        purple_line_length = np.linalg.norm(furthest_point - start_point)  # Distance between start and furthest point (purple line)\n",
    "        third_line_length = np.linalg.norm(furthest_point - end_of_red_line)  # Distance between end of red line and furthest point (third line)\n",
    "\n",
    "        # Using the cosine rule to calculate the angle between the red and purple lines\n",
    "        cos_theta = (red_line_length**2 + purple_line_length**2 - third_line_length**2) / (2 * red_line_length * purple_line_length)\n",
    "        angle_radians = np.arccos(np.clip(cos_theta, -1.0, 1.0))  # Clip value to avoid out-of-bound errors\n",
    "        angle_degrees = np.degrees(angle_radians)  # Convert radians to degrees\n",
    "        \n",
    "        return angle_degrees\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "def line(events, event_idx):\n",
    "    w_hits = np.array(events.reco_hits_w[event_idx])\n",
    "    x_hits = np.array(events.reco_hits_x_w[event_idx])\n",
    "\n",
    "    if len(w_hits) == len(x_hits) and len(w_hits) > 15:\n",
    "    \n",
    "        # Calculate differences between consecutive points\n",
    "        dx = np.diff(w_hits)\n",
    "        dy = np.diff(x_hits)\n",
    "    \n",
    "        # Compute segment lengths\n",
    "        segment_lengths = np.sqrt(dx**2 + dy**2)\n",
    "    \n",
    "        # Total arc length (line integral)\n",
    "        total_length = np.sum(segment_lengths)\n",
    "    \n",
    "        # Normalize by the number of points\n",
    "        normalised_length = total_length / len(w_hits)\n",
    "\n",
    "        return normalised_length\n",
    "    else:\n",
    "        return None \n",
    "        \n",
    "def q4(events, event_idx):\n",
    "    adcs = events.reco_adcs_w[event_idx]\n",
    "\n",
    "    if len(adcs) > 15:\n",
    "\n",
    "        q4_idx = len(adcs) // 4\n",
    "\n",
    "        adcs_q4 = adcs[-q4_idx:]\n",
    "    \n",
    "        ratio = sum(adcs_q4) / sum(adcs)\n",
    "    \n",
    "    \n",
    "        return ratio\n",
    "    else:\n",
    "       return None\n",
    "\n",
    "# e/gamma\n",
    "def step_length(events, event_idx):\n",
    "    # Find all info for the feature\n",
    "    w_hits = events.reco_hits_w[event_idx]\n",
    "    x_hits = events.reco_hits_x_w[event_idx]\n",
    "    w_vtx = events.neutrino_vtx_w[event_idx]\n",
    "    x_vtx = events.neutrino_vtx_x[event_idx]\n",
    "\n",
    "    # Skip events where there are fewer than 15 hits\n",
    "    if len(w_hits) < 15:\n",
    "        return None  # Return None to indicate that the feature should be skipped\n",
    "\n",
    "    # Finding step length\n",
    "    w_step = min([abs(w - w_vtx) for w in w_hits])\n",
    "    x_step = min([abs(x - x_vtx) for x in x_hits])\n",
    "    step_length = np.sqrt(w_step**2 + x_step**2)\n",
    "\n",
    "    return step_length\n",
    "\n",
    "def find_radial_density_increase(x, y, bins=50, center=None, start_radius=0, debug=False, safety_r=5): # NOT A FEATURE\n",
    "    if center is None:\n",
    "        center = (np.mean(x), np.mean(y))\n",
    "    \n",
    "    r = np.sqrt((x - center[0])**2 + (y - center[1])**2)\n",
    "    \n",
    "    r = r[r > start_radius]\n",
    "    \n",
    "    if len(r) == 0:\n",
    "        return safety_r\n",
    "    \n",
    "    r_sorted = np.sort(r)\n",
    "    \n",
    "    bin_edges = np.linspace(start_radius, max(r_sorted), bins)\n",
    "    counts, _ = np.histogram(r_sorted, bins=bin_edges)\n",
    "    \n",
    "    areas = np.pi * (bin_edges[1:]**2 - bin_edges[:-1]**2)\n",
    "    densities = counts / areas\n",
    "    \n",
    "    diffs = np.diff(densities)\n",
    "    increase_idx = np.argmax(diffs > 0)\n",
    "    \n",
    "    if increase_idx == 0 and diffs[0] <= 0:\n",
    "        return safety_r\n",
    "    \n",
    "    return bin_edges[increase_idx]\n",
    "def dE_dx(events, event_idx, smear=[0, 0], cone_angle=(5/6)*np.pi, debug=False):\n",
    "    w_hits = events.reco_hits_w[event_idx]\n",
    "\n",
    "    if len(w_hits) <= 15:\n",
    "        if debug:\n",
    "            print(f'Event {event_idx} does not meet 15 hit cutoff\\n')\n",
    "        return None\n",
    "\n",
    "    def debug_print(message):\n",
    "        if debug:\n",
    "            print(f\"    {message}\")\n",
    "    if debug:\n",
    "        print(f\"Debug for event {event_idx}\")\n",
    "    \n",
    "    x_hits = events.reco_hits_x_w[event_idx]\n",
    "    w_vtx = events.neutrino_vtx_w[event_idx]\n",
    "    x_vtx = events.neutrino_vtx_x[event_idx]\n",
    "    w_vtx_smeared = w_vtx + smear[0]\n",
    "    x_vtx_smeared = x_vtx + smear[1]\n",
    "    adcs = events.reco_adcs_w[event_idx]\n",
    "    \n",
    "    pdg = events.mc_pdg[event_idx]\n",
    "    c_class = 'lightblue'\n",
    "    event = 'ERROR: Not e/gamma'\n",
    "    if pdg in [-11, 11]:\n",
    "        c_class = '#EEEE00'\n",
    "        event = 'Electron'\n",
    "    elif pdg == 22:\n",
    "        c_class = 'g'\n",
    "        event = 'Photon'\n",
    "    debug_print(f'Lepton: {event}')\n",
    "\n",
    "    if np.sign(np.mean(w_hits) - w_vtx) == -1:\n",
    "        w_hits = 2 * w_vtx - np.array(w_hits)\n",
    "        debug_print(f'Event {event_idx} was left-facing, and has been inverted')\n",
    "\n",
    "    theta_0 = np.arctan2(np.mean(x_hits) - x_vtx, np.mean(w_hits) - w_vtx)\n",
    "    theta_u = theta_0 + (cone_angle/2)\n",
    "    theta_l = theta_0 - (cone_angle/2)\n",
    "    debug_print(f'Angle info:\\n        Yaw = {np.degrees(theta_0):.2f}° from +w\\n        Upper Angle = {np.degrees(theta_u):.2f}°\\n        Lower Angle: {np.degrees(theta_l):.2f}°')\n",
    "    \n",
    "    # You may also want to play around with cone_angle, I don't expect it to have a super huge effect for values > pi/2\n",
    "\n",
    "    angles = np.arctan2(x_hits - x_vtx_smeared, w_hits - w_vtx_smeared)\n",
    "    distance = [np.sqrt((w - w_vtx)**2 + (x - x_vtx)**2) for w, x in zip(w_hits, x_hits)]\n",
    "\n",
    "    r_start = 5\n",
    "    if len(distance) >= 5:\n",
    "        r_start = sorted(distance)[4]\n",
    "        \n",
    "    # r_start is arbitrary, just the distance from the event vertex that the radial density algorithm will begin searching, its good for events that start with a cluster and then a gap\n",
    "    # worth tweaking if you really want to\n",
    "    \n",
    "    testing_distance = find_radial_density_increase(w_hits, x_hits, center = (w_vtx_smeared, x_vtx_smeared), start_radius = r_start)\n",
    "    branch_distance = max(r_start + 5, testing_distance)\n",
    "    \n",
    "    mask = (angles >= theta_l) & (angles <= theta_u) & (distance < branch_distance)\n",
    "    if mask.sum() == 0:\n",
    "        debug_print('ERROR: Mask is empty')\n",
    "        return None\n",
    "\n",
    "    points = np.column_stack((w_hits[mask], x_hits[mask]))\n",
    "    di = np.sqrt(((points[:, None, :] - points[None, :, :]) ** 2).sum(axis=2))\n",
    "    dx = np.maximum(np.max(di), 0.48)\n",
    "\n",
    "    val = np.sum(adcs[mask]) / dx\n",
    "    debug_print(f'dE = {np.sum(adcs[mask])}')\n",
    "    debug_print(f'dx = {dx}')\n",
    "    debug_print(f'dE/dx = {val}\\n')\n",
    "\n",
    "    return val\n",
    "\n",
    "# booster features\n",
    "def hit_count(events, event_idx, min_hits=15):\n",
    "    \"\"\" Hit_count is to help candidate_lepton it increased the accuracy by around 1.5% \"\"\"\n",
    "    # Get all hits for the given event\n",
    "    all_w_hits = events.reco_hits_w[event_idx]  # Wire plane hits\n",
    "    all_x_hits = events.reco_hits_x_w[event_idx]  # X-plane hits\n",
    "    \n",
    "    # Count total number of hits\n",
    "    num_hits = len(all_w_hits)  # Assuming same length for both planes\n",
    "\n",
    "    # Apply filtering: Skip events with fewer than `min_hits`\n",
    "    if num_hits < min_hits:\n",
    "        return None  # Not enough hits\n",
    "    \n",
    "    return num_hits\n",
    "\n",
    "def adc_sum(events, event_idx):\n",
    "    adcs = events.reco_adcs_w[event_idx]\n",
    "    return np.sum(adcs)\n",
    "\n",
    "def hull_density(events, event_idx):\n",
    "    w_hits = events.reco_hits_w[event_idx] \n",
    "\n",
    "    if len(w_hits) < 15:\n",
    "        return None\n",
    "        \n",
    "    x_hits = events.reco_hits_x_w[event_idx]\n",
    "    points = np.column_stack((w_hits, x_hits))\n",
    "    \n",
    "    hull = ConvexHull(points)\n",
    "    hull_area = hull.volume\n",
    "    \n",
    "    hull_density = len(points) / hull_area\n",
    "    return hull_density\n",
    "\n",
    "# candidate lepton feature\n",
    "def primary(events, idx_array): # NOT A FEATURE\n",
    "    # find nu_vtx and particle_vtx of all indices in idx_array\n",
    "    nu_vtx_w = events.neutrino_vtx_w[idx_array]\n",
    "    nu_vtx_x = events.neutrino_vtx_x[idx_array]\n",
    "    idx_w_vtx = events.reco_particle_vtx_w[idx_array]\n",
    "    idx_x_vtx = events.reco_particle_vtx_x[idx_array]\n",
    "\n",
    "    w_distances = idx_w_vtx - nu_vtx_w\n",
    "    x_distances = idx_x_vtx - nu_vtx_x\n",
    "\n",
    "    abs_distances = np.sqrt(w_distances**2 + x_distances**2)\n",
    "\n",
    "    # if distances are empty\n",
    "    if len(abs_distances) != 0:\n",
    "        primary = np.argmin(abs_distances)\n",
    "        return idx_array[primary]\n",
    "    else: return None\n",
    "def charged_candidate_feature(events, event_idx):\n",
    "    identifiers = events.event_number\n",
    "    change_indices = np.where(np.diff(identifiers) != 0)[0] + 1\n",
    "    slices = np.split(np.arange(len(identifiers)), change_indices)  # Groups indices by event\n",
    "    \n",
    "    # Find the subarray that contains event_idx\n",
    "    event_indices = next((sub for sub in slices if event_idx in sub), None)\n",
    "\n",
    "    # If event_idx is not found, return None\n",
    "    if event_indices is None:\n",
    "        return None\n",
    "\n",
    "    candidate_idx = primary(events, event_indices)  # Identify primary candidate\n",
    "    \n",
    "    if events.mc_pdg[candidate_idx] in [-13, 13]:\n",
    "        return 2  # Primary muon detected\n",
    "    elif events.mc_pdg[candidate_idx] in [-11, 11]:\n",
    "        return 1  # Primary electron detected\n",
    "    else: return 0 # No useful primary candidate found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4eed235-7dbf-40dc-8ed9-7479339b095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Want to get an overall look at the feature for an event\n",
    "-> this function gets, for one feature, the mean, median, standard deviation, max, min and sum of the features\n",
    "-> returns this as a LIST of 6 values\n",
    "\"\"\"\n",
    "\n",
    "def find_feature_stats(feature_func, events, event_number):\n",
    "    identifiers = events.event_number\n",
    "    change_indices = np.where(np.diff(identifiers) != 0)[0] + 1\n",
    "    event_indices = np.split(np.arange(len(identifiers)), change_indices)[event_number]\n",
    "\n",
    "    event_feature_values = [feature_func(events, i) for i in event_indices]\n",
    "    event_feature_values = [v for v in event_feature_values if v is not None]\n",
    "\n",
    "    # If event_feature_values is empty after filtering, return [None] * 6\n",
    "    if len(event_feature_values) == 0:\n",
    "        return [None] * 6\n",
    "        \n",
    "    stats = [\n",
    "        np.mean(event_feature_values),\n",
    "        np.median(event_feature_values),\n",
    "        np.std(event_feature_values),\n",
    "        np.max(event_feature_values),\n",
    "        np.min(event_feature_values),\n",
    "        np.sum(event_feature_values),\n",
    "    ]\n",
    "\n",
    "    return stats\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_x_data(feature_func_array, events): # calculates feature data for input into a bdt.\n",
    "    n = len(np.unique(events.event_number))\n",
    "    x_data = []\n",
    "\n",
    "    # Use tqdm to add a progress bar to the loop\n",
    "    for event_number in tqdm(range(n), desc=\"Processing Events\", unit=\"event\"):\n",
    "        x = []\n",
    "        for func in feature_func_array:\n",
    "            stats = find_feature_stats(func, events, event_number)\n",
    "            x.extend(stats)\n",
    "\n",
    "        x_data.append(x)  # Append to x_data\n",
    "\n",
    "    return np.array(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71a99094-c43a-428e-ac22-4293e15e4330",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we need to label the data\n",
    "The BDT takes in 12 (features) * 6 (stats) values, \n",
    "=> each event_number will have 72 features\n",
    "\n",
    "Conventionally label events as so for y_train data:\n",
    "    - 2 (CCnu_mu)\n",
    "    - 1 (CCnu_e)\n",
    "    - 0 (NCnu_x)\n",
    "\"\"\"\n",
    "\n",
    "def label_events(events):\n",
    "    identifiers = events.event_number\n",
    "    change_indices = np.where(np.diff(identifiers) != 0)[0] + 1\n",
    "    first_idx = np.array([arr[0] for arr in np.split(np.arange(len(identifiers)), change_indices)])  # Fix here\n",
    "\n",
    "    numu = events.is_numu[first_idx]\n",
    "    nue = events.is_nue[first_idx]\n",
    "    \n",
    "    y_data = np.zeros_like(first_idx)\n",
    "    y_data[numu] = 2\n",
    "    y_data[nue] = 1\n",
    "    \n",
    "    return y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1764d8eb-a52a-400a-a21c-d1bf85029bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The BDT:\n",
    "    1. create the feature_func_array\n",
    "    2. get x_train and y_train, x_test, y_test\n",
    "    3. hyperparametrise the bdt\n",
    "    4. save the bdt\n",
    "\"\"\"\n",
    "features = [ # Step 1\n",
    "    correlation,\n",
    "    noise,\n",
    "    rms,\n",
    "    angle,\n",
    "    line,\n",
    "    q4,\n",
    "    step_length,\n",
    "    dE_dx,\n",
    "    hit_count,\n",
    "    adc_sum,\n",
    "    hull_density,\n",
    "    charged_candidate_feature\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1323d95-33eb-4e52-a841-327849c820d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "No data left in file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mLoading the Training and Testing Data\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m trained_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_data_72f.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 6\u001b[0m testing_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtesting_data_72f.npz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m x_trained \u001b[38;5;241m=\u001b[39m trained_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_train\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      9\u001b[0m y_trained \u001b[38;5;241m=\u001b[39m trained_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_train\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/numpy/lib/npyio.py:436\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    434\u001b[0m magic \u001b[38;5;241m=\u001b[39m fid\u001b[38;5;241m.\u001b[39mread(N)\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m magic:\n\u001b[0;32m--> 436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data left in file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m# If the file size is less than N, we need to make sure not\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m# to seek past the beginning of the file\u001b[39;00m\n\u001b[1;32m    439\u001b[0m fid\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mmin\u001b[39m(N, \u001b[38;5;28mlen\u001b[39m(magic)), \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# back-up\u001b[39;00m\n",
      "\u001b[0;31mEOFError\u001b[0m: No data left in file"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Loading the Training and Testing Data\n",
    "\"\"\"\n",
    "\n",
    "trained_data = np.load(\"training_data_72f.npz\", allow_pickle=True)\n",
    "testing_data = np.load(\"testing_data_72f.npz\", allow_pickle=True)\n",
    "\n",
    "x_trained = trained_data[\"x_train\"]\n",
    "y_trained = trained_data[\"y_train\"]\n",
    "\n",
    "x_testing = testing_data[\"x_test\"]\n",
    "y_testing = testing_data[\"y_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b377f64-b0ea-486d-898d-35f364faa9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Step 3 \"\"\"\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 500],  # Number of trees\n",
    "    'max_depth': [3, 5, 7, 10],  # Tree depth\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 0.3],  # Step size shrinkage\n",
    "    'subsample': [0.6, 0.8, 1.0],  # Row sampling per tree\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],  # Feature sampling per tree\n",
    "    'gamma': [0, 0.1, 0.2, 0.5],  # Minimum loss reduction for split\n",
    "    'reg_lambda': [0, 1, 10],  # L2 regularization\n",
    "    'reg_alpha': [0, 1, 10]  # L1 regularization\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0da55c-acf1-4dd8-90fc-522931f41960",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Step 4 \"\"\"\n",
    "\n",
    "bdt = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "\n",
    "# Use Randomized Search for hyperparameter tuning\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=bdt,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50,  # Number of random samples to try\n",
    "    scoring='accuracy',  # Optimize for classification accuracy\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1,  # Use all available CPU cores\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train with hyperparameter tuning\n",
    "random_search.fit(x_trained, y_trained)\n",
    "\n",
    "# Get the best model and its parameters\n",
    "best_xgb = random_search.best_estimator_\n",
    "print(\"Best hyperparameters found: \", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef276836-015b-4519-b7f4-0203ce0242f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
