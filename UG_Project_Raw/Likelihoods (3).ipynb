{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5c65b4a-04eb-4036-9d59-23b5d391521d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: uproot in /opt/conda/lib/python3.11/site-packages (5.5.1)\n",
      "Requirement already satisfied: awkward in /opt/conda/lib/python3.11/site-packages (2.7.2)\n",
      "Requirement already satisfied: cramjam>=2.5.0 in /opt/conda/lib/python3.11/site-packages (from uproot) (2.9.1)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from uproot) (2024.10.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from uproot) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from uproot) (24.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from uproot) (3.5.0)\n",
      "Requirement already satisfied: awkward-cpp==43 in /opt/conda/lib/python3.11/site-packages (from awkward) (43)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /opt/conda/lib/python3.11/site-packages (from awkward) (7.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata>=4.13.0->awkward) (3.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install uproot awkward \n",
    "from uproot_io import Events, View\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c12a6b8-386a-4d75-8784-e347462516aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: /home/jovyan/CheatedRecoFile_0.root\n",
      "File size: 575473011 bytes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = \"/home/jovyan/CheatedRecoFile_1.root\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    print(f\"File exists: {file_path}\")\n",
    "    print(f\"File size: {os.path.getsize(file_path)} bytes\")\n",
    "else:\n",
    "    print(f\"File does not exist: {file_path}\")\n",
    "\n",
    "events_unseen = Events(\"CheatedRecoFile_1.root\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3781dddc-932c-423e-8f02-34851f19986d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'histogram_data.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the histogram data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m correlation_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhistogram_data.npz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Load correlation histogram data\u001b[39;00m\n\u001b[1;32m      3\u001b[0m noise_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdbscan_histogram_data.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Load noise histogram data\u001b[39;00m\n\u001b[1;32m      4\u001b[0m rms_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrms_pdf_histogram.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Load rms histogram data\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'histogram_data.npz'"
     ]
    }
   ],
   "source": [
    "# Load the histogram data\n",
    "correlation_data = np.load(\"histogram_data.npz\")  # Load correlation histogram data\n",
    "noise_data = np.load(\"dbscan_histogram_data.npz\")  # Load noise histogram data\n",
    "rms_data = np.load(\"rms_pdf_histogram.npz\")  # Load rms histogram data\n",
    "angles_data = np.load(\"angles_histogram_data.npz\")  # Load rms histogram data\n",
    "line_data = np.load(\"line_integrals_histogram_data.npz\")  # Load rms histogram data\n",
    "q4_data = np.load(\"adc_q4_histogram_data.npz\")\n",
    "\n",
    "# Extract correlation histogram details\n",
    "correlation_bin_edges = correlation_data['track_bin_edges']  # Use track bin edges\n",
    "correlation_track_counts = correlation_data['track_counts']\n",
    "correlation_shower_counts = correlation_data['shower_counts']\n",
    "\n",
    "# Extract noise histogram details\n",
    "noise_bin_edges = noise_data['track_bin_edges']  # Use track bin edges\n",
    "noise_track_counts = noise_data['track_counts']\n",
    "noise_shower_counts = noise_data['shower_counts']\n",
    "\n",
    "rms_bin_edges = noise_data['track_bin_edges']  # Use track bin edges\n",
    "rms_track_counts = noise_data['track_counts']\n",
    "rms_shower_counts = noise_data['shower_counts']\n",
    " # histogram of number of hits versus cuts \n",
    "\n",
    "angles_bin_edges = angles_data['track_bin_edges']  # Use track bin edges\n",
    "angles_track_counts = angles_data['track_counts']\n",
    "angles_shower_counts = angles_data['shower_counts']\n",
    "\n",
    "line_bin_edges = line_data['track_bin_edges']  # Use track bin edges\n",
    "line_track_counts = line_data['track_counts']\n",
    "line_shower_counts = line_data['shower_counts']\n",
    "\n",
    "q4_bin_edges = line_data['track_bin_edges']  # Use track bin edges\n",
    "q4_track_counts = line_data['track_counts']\n",
    "q4_shower_counts = line_data['shower_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92fe9ea-1989-4922-b87e-63b09a511b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from before \n",
    "def correlation(events, event_idx):\n",
    "    x_hits = events.reco_hits_x_w[event_idx]\n",
    "    w_hits = events.reco_hits_w[event_idx] \n",
    "\n",
    "    # Check if there are valid hits\n",
    "    if len(w_hits) == len(x_hits) and len(w_hits) > 15: # talk about advantages and disadvantages of results with a threshold \n",
    "        if np.std(x_hits) == 0 or np.std(w_hits) == 0:\n",
    "            return None  # No valid correlation if there's no variation in data\n",
    "        \n",
    "        correlation = np.corrcoef(x_hits, w_hits)[0, 1]\n",
    "        \n",
    "        # Fit line using w_hits for x and calculate predicted y-values\n",
    "        line_fit = np.polyfit(w_hits, x_hits, 1)\n",
    "        line_y_pred = np.polyval(line_fit, w_hits)\n",
    "        \n",
    "        # Calculate line error between predicted and actual x_hits\n",
    "        line_error = np.mean((x_hits - line_y_pred) ** 2)\n",
    "        \n",
    "        # Normalize scores\n",
    "        correlation_score = abs(correlation) if not np.isnan(correlation) else 0\n",
    "        error_score = max(0, 1 - line_error / 20) if line_error < 20 else 0\n",
    "        \n",
    "        # Weighted score\n",
    "        line_score = (correlation_score * 0.7) + (error_score * 0.3)\n",
    "        \n",
    "        return (line_score * 100)  # Return the score and category\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# from before \n",
    "\n",
    "def noise(events, event_idx, eps=2, min_samples=5):\n",
    "    # Extract hit positions (no PDG filtering, just use reco hits)\n",
    "    x_hits = events.reco_hits_x_w[event_idx]\n",
    "    w_hits = events.reco_hits_w[event_idx]\n",
    "\n",
    "    # Check if there are valid hits\n",
    "    if len(w_hits) == len(x_hits) and len(w_hits) > 15:\n",
    "        # Combine the coordinates for clustering\n",
    "        hits_coordinates = np.column_stack((w_hits, x_hits))\n",
    "\n",
    "        # Apply DBSCAN clustering\n",
    "        db = DBSCAN(eps=eps, min_samples=min_samples).fit(hits_coordinates)\n",
    "        labels = db.labels_\n",
    "\n",
    "        # Count noise points (labeled as -1)\n",
    "        n_noise = np.sum(labels == -1)\n",
    "\n",
    "        # Count clusters (unique labels excluding -1)\n",
    "        unique_clusters = set(labels) - {-1}\n",
    "        n_clusters = len(unique_clusters)\n",
    "\n",
    "        return n_noise + n_clusters\n",
    "    else:\n",
    "        return None\n",
    "def rms(events, event_idx):\n",
    "    w_hits = events.reco_hits_w[event_idx]\n",
    "    x_hits = events.reco_hits_x_w[event_idx]\n",
    "\n",
    "    if len(w_hits) == len(x_hits) and len(w_hits) > 15:\n",
    "        slope, intercept = np.polyfit(w_hits, x_hits, 1)\n",
    "        \n",
    "        actual = x_hits\n",
    "        predicted = slope * w_hits + intercept\n",
    "        \n",
    "        meanSquaredError = ((predicted - actual) ** 2).mean()\n",
    "        return np.sqrt(meanSquaredError)\n",
    "    else: \n",
    "        return None \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def angle(events, event_idx):\n",
    "    x_hits = events.reco_hits_x_w[event_idx]\n",
    "    w_hits = events.reco_hits_w[event_idx]\n",
    "\n",
    "    if len(w_hits) == len(x_hits) and len(w_hits) > 15:\n",
    "        # Fit the best-fit line\n",
    "        line_fit = np.polyfit(w_hits, x_hits, 1)\n",
    "        line_slope = line_fit[0]\n",
    "        line_intercept = line_fit[1]\n",
    "\n",
    "        # Calculate residuals (distance from the line)\n",
    "        line_y_pred = np.polyval(line_fit, w_hits)\n",
    "        residuals = np.abs(x_hits - line_y_pred)\n",
    "\n",
    "        # Find the index of the furthest point\n",
    "        furthest_idx = np.argmax(residuals)\n",
    "        furthest_point = np.array([x_hits[furthest_idx], w_hits[furthest_idx]])\n",
    "\n",
    "        # Start of the line is at the minimum W-coordinate\n",
    "        min_w = np.min(w_hits)\n",
    "        start_point = np.array([line_slope * min_w + line_intercept, min_w])\n",
    "\n",
    "        # End of the red line (best-fit line) at the maximum W-coordinate\n",
    "        max_w = np.max(w_hits)\n",
    "        end_of_red_line = np.array([line_slope * max_w + line_intercept, max_w])\n",
    "\n",
    "        # Calculate the lengths of the three sides of the triangle\n",
    "        red_line_length = np.linalg.norm(end_of_red_line - start_point)  # Distance between start and end of red line\n",
    "        purple_line_length = np.linalg.norm(furthest_point - start_point)  # Distance between start and furthest point (purple line)\n",
    "        third_line_length = np.linalg.norm(furthest_point - end_of_red_line)  # Distance between end of red line and furthest point (third line)\n",
    "\n",
    "        # Using the cosine rule to calculate the angle between the red and purple lines\n",
    "        cos_theta = (red_line_length**2 + purple_line_length**2 - third_line_length**2) / (2 * red_line_length * purple_line_length)\n",
    "        angle_radians = np.arccos(np.clip(cos_theta, -1.0, 1.0))  # Clip value to avoid out-of-bound errors\n",
    "        angle_degrees = np.degrees(angle_radians)  # Convert radians to degrees\n",
    "        \n",
    "        return angle_degrees\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def line(events, event_idx):\n",
    "    w_hits = np.array(events.reco_hits_w[event_idx])\n",
    "    x_hits = np.array(events.reco_hits_x_w[event_idx])\n",
    "\n",
    "    if len(w_hits) == len(x_hits) and len(w_hits) > 15:\n",
    "    \n",
    "        # Calculate differences between consecutive points\n",
    "        dx = np.diff(w_hits)\n",
    "        dy = np.diff(x_hits)\n",
    "    \n",
    "        # Compute segment lengths\n",
    "        segment_lengths = np.sqrt(dx**2 + dy**2)\n",
    "    \n",
    "        # Total arc length (line integral)\n",
    "        total_length = np.sum(segment_lengths)\n",
    "    \n",
    "        # Normalize by the number of points\n",
    "        normalised_length = total_length / len(w_hits)\n",
    "\n",
    "        return normalised_length\n",
    "    else:\n",
    "        return None \n",
    "\n",
    "\n",
    "\n",
    "def q4(events, event_idx):\n",
    "    adcs = events.reco_adcs_w[event_idx]\n",
    "\n",
    "    if len(adcs) > 15:\n",
    "\n",
    "        q4_idx = len(adcs) // 4\n",
    "\n",
    "        adcs_q4 = adcs[-q4_idx:]\n",
    "    \n",
    "        ratio = sum(adcs_q4) / sum(adcs)\n",
    "    \n",
    "    \n",
    "        return ratio\n",
    "    else:\n",
    "       return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4ddba2-cc2f-4066-b440-2aa8c81419c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shower_pdg_set = {11, -11, 22, -22}  # Define PDG codes for showers\n",
    "def get_probabilities_from_histogram(score, bin_edges, track_counts, shower_counts):\n",
    "    \"\"\"Returns both track and shower probabilities for a given score\"\"\"\n",
    "    bin_index = np.digitize(score, bin_edges) - 1  # Find the bin index for the score\n",
    "    if bin_index < 0 or bin_index >= len(track_counts):\n",
    "        return 0, 0  # If score is out of range, return 0 for both probabilities\n",
    "    return track_counts[bin_index], shower_counts[bin_index]  # Return both probabilities\n",
    "\n",
    "def categorise_event(events, event_idx, shower_pdg_set):\n",
    "    # Extract PDG code\n",
    "    pdg_code = events.mc_pdg[event_idx]  # Assuming this holds a single PDG code\n",
    "    \n",
    "    # Determine if event is a shower or track\n",
    "    if pdg_code in shower_pdg_set:\n",
    "        return 'shower'\n",
    "    else:\n",
    "        return 'track'  # All other PDG codes are considered as track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233f651b-ce88-44f5-a1c4-ea84baeb24e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong Likelihood Approch - Correct one Below\n",
    "def likelihood_approach_wrong(events, event_idx):\n",
    "    \"\"\"Applies the likelihood approach for an unseen event.\"\"\"\n",
    "    # Calculate correlation and noise scores\n",
    "    correlation_score = correlation(events, event_idx)\n",
    "    noise_score = noise(events, event_idx)\n",
    "    rms_score = rms(events, event_idx)\n",
    "    angle_score = angle(events, event_idx)\n",
    "    line_score = line(events, event_idx)\n",
    "    q4_score = q4(events, event_idx)\n",
    "    \n",
    "    # Ensure valid scores\n",
    "    if correlation_score is None or noise_score is None:\n",
    "        return None\n",
    "\n",
    "    # Get probabilities from correlation histogram\n",
    "    prob_correlation_track, prob_correlation_shower = get_probabilities_from_histogram(\n",
    "        correlation_score, correlation_bin_edges, correlation_track_counts, correlation_shower_counts\n",
    "    )\n",
    "\n",
    "    # Get probabilities from noise histogram\n",
    "    prob_noise_track, prob_noise_shower = get_probabilities_from_histogram(\n",
    "        noise_score, noise_bin_edges, noise_track_counts, noise_shower_counts\n",
    "    )\n",
    "\n",
    "    # Get probabilities from RMS histogram\n",
    "    prob_rms_track, prob_rms_shower = get_probabilities_from_histogram(\n",
    "        rms_score, rms_bin_edges, rms_track_counts, rms_shower_counts\n",
    "    )\n",
    "\n",
    "    # Get probabilities from RMS histogram\n",
    "    prob_angle_track, prob_angle_shower = get_probabilities_from_histogram(\n",
    "        angle_score, angles_bin_edges, angles_track_counts, angles_shower_counts\n",
    "    )\n",
    "\n",
    "    # Get probabilities from RMS histogram\n",
    "    prob_line_track, prob_line_shower = get_probabilities_from_histogram(\n",
    "        line_score, line_bin_edges, line_track_counts, line_shower_counts\n",
    "    )\n",
    "\n",
    "    # Get probabilities from RMS histogram\n",
    "    prob_q4_track, prob_q4_shower = get_probabilities_from_histogram(\n",
    "        q4_score, q4_bin_edges, q4_track_counts, q4_shower_counts\n",
    "    )\n",
    "\n",
    "    # Compute likelihoods\n",
    "    likelihood_track = prob_correlation_track * prob_noise_track * prob_rms_track * prob_angle_track * prob_line_track * prob_q4_track\n",
    "    likelihood_shower = prob_correlation_shower * prob_noise_shower * prob_rms_shower * prob_angle_shower * prob_line_shower * prob_q4_shower\n",
    "\n",
    "    \n",
    "    # Normalize to get posterior probabilities\n",
    "    total_likelihood = likelihood_track + likelihood_shower\n",
    "    if total_likelihood == 0:\n",
    "        return None\n",
    "\n",
    "    Track_L = likelihood_track / total_likelihood\n",
    "    Shower_L = likelihood_shower / total_likelihood\n",
    "\n",
    "    # Determine preliminary classification using the categorise_event function\n",
    "    prelim_classification = categorise_event(events, event_idx, shower_pdg_set)\n",
    "\n",
    "    # Return only the relevant likelihood based on preliminary classification\n",
    "    if prelim_classification == \"track\":\n",
    "        return \"Track\", Track_L, prob_correlation_track, prob_noise_track, prob_rms_track, prob_angle_track, prob_line_track, prob_q4_track\n",
    "    else:\n",
    "        return \"Shower\", Shower_L, prob_correlation_shower, prob_noise_shower, prob_rms_shower, prob_angle_shower, prob_line_shower, prob_q4_shower\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdf7f4c-e439-478d-aee8-c2dfe9c14cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Scores from correct Likelihood\n",
    "def likelihood_approach(events, event_idx, shower_pdg_set):\n",
    "    \"\"\"Applies the likelihood approach for an unseen event.\"\"\"\n",
    "    # Calculate correlation and noise scores\n",
    "    correlation_score = correlation(events, event_idx)\n",
    "    noise_score = noise(events, event_idx)\n",
    "    rms_score = rms(events, event_idx)\n",
    "    angle_score = angle(events, event_idx)\n",
    "    line_score = line(events, event_idx)\n",
    "    q4_score = q4(events, event_idx)\n",
    "\n",
    "    # Ensure valid scores\n",
    "    if correlation_score is None or noise_score is None:\n",
    "        return None\n",
    "\n",
    "    # Get probabilities from correlation histogram\n",
    "    prob_correlation_track, prob_correlation_shower = get_probabilities_from_histogram(\n",
    "        correlation_score, correlation_bin_edges, correlation_track_counts, correlation_shower_counts\n",
    "    )\n",
    "\n",
    "    # Get probabilities from noise histogram\n",
    "    prob_noise_track, prob_noise_shower = get_probabilities_from_histogram(\n",
    "        noise_score, noise_bin_edges, noise_track_counts, noise_shower_counts\n",
    "    )\n",
    "\n",
    "    # Get probabilities from RMS histogram\n",
    "    prob_rms_track, prob_rms_shower = get_probabilities_from_histogram(\n",
    "        rms_score, rms_bin_edges, rms_track_counts, rms_shower_counts\n",
    "    )\n",
    "\n",
    "    # Get probabilities from angle histogram\n",
    "    prob_angle_track, prob_angle_shower = get_probabilities_from_histogram(\n",
    "        angle_score, angles_bin_edges, angles_track_counts, angles_shower_counts\n",
    "    )\n",
    "\n",
    "    # Get probabilities from line histogram\n",
    "    prob_line_track, prob_line_shower = get_probabilities_from_histogram(\n",
    "        line_score, line_bin_edges, line_track_counts, line_shower_counts\n",
    "    )\n",
    "\n",
    "    # Get probabilities from q4 histogram\n",
    "    prob_q4_track, prob_q4_shower = get_probabilities_from_histogram(\n",
    "        q4_score, q4_bin_edges, q4_track_counts, q4_shower_counts\n",
    "    )\n",
    "\n",
    "    # Compute likelihoods\n",
    "    likelihood_track = (prob_correlation_track * prob_noise_track * prob_rms_track *\n",
    "                        prob_angle_track * prob_line_track * prob_q4_track)\n",
    "    likelihood_shower = (prob_correlation_shower * prob_noise_shower * prob_rms_shower *\n",
    "                         prob_angle_shower * prob_line_shower * prob_q4_shower)\n",
    "\n",
    "    # Normalize to get posterior probabilities\n",
    "    total_likelihood = likelihood_track + likelihood_shower\n",
    "    if total_likelihood == 0:\n",
    "        return None\n",
    "\n",
    "    Track_L = likelihood_track / total_likelihood\n",
    "    Shower_L = likelihood_shower / total_likelihood\n",
    "\n",
    "    # Determine preliminary classification using the categorise_event function\n",
    "    prelim_classification = categorise_event(events, event_idx, shower_pdg_set)\n",
    "\n",
    "    # Return Track_L and its classification\n",
    "    return Track_L, Shower_L, prelim_classification\n",
    "\n",
    "# Plotting the Likelihood Histograms\n",
    "def plot_histograms(track_likelihoods):\n",
    "    \"\"\"Plots histograms of Track_L and Track_S values for tracks and showers.\"\"\"\n",
    "    # Separate Track_L and Track_S values by classification\n",
    "    track_l_values = [track_l for track_l, track_s, category in track_likelihoods if category == 'track']\n",
    "    shower_l_values = [track_l for track_l, track_s, category in track_likelihoods if category == 'shower']\n",
    "    track_s_values = [track_s for track_l, track_s, category in track_likelihoods if category == 'track']\n",
    "    shower_s_values = [track_s for track_l, track_s, category in track_likelihoods if category == 'shower']\n",
    "\n",
    "    # Plot Track_L\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(track_l_values, bins=30, color='red', alpha=0.7, label='Track (Track_L)')\n",
    "    plt.hist(shower_l_values, bins=30, color='blue', alpha=0.7, label='Shower (Track_L)')\n",
    "    plt.xlabel('Track_L')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Track_L for Tracks and Showers')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Track_S\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(track_s_values, bins=30, color='red', alpha=0.7, label='Track (Track_S)')\n",
    "    plt.hist(shower_s_values, bins=30, color='blue', alpha=0.7, label='Shower (Track_S)')\n",
    "    plt.xlabel('Shower_L')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Track_S for Tracks and Showers')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Main script to process events and plot\n",
    "likelihoods = []  # List to store (Track_L, Track_S, classification) for all events\n",
    "\n",
    "# Process each event\n",
    "for event_idx in range(len(events_unseen.reco_hits_w)):\n",
    "    result = likelihood_approach(events_unseen, event_idx, shower_pdg_set)\n",
    "    if result is not None:\n",
    "        likelihoods.append(result)\n",
    "\n",
    "# Plot the histograms\n",
    "plot_histograms(likelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7bb9e3-1dd2-446f-bb2e-b2e0b8ce67bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms_log(track_likelihoods):\n",
    "    \"\"\"Plots histograms of Track_L and Track_S values for tracks and showers, with log-scaled versions.\"\"\"\n",
    "    # Separate Track_L and Track_S values by classification\n",
    "    track_l_values = [track_l for track_l, track_s, category in track_likelihoods if category == 'track']\n",
    "    shower_l_values = [track_l for track_l, track_s, category in track_likelihoods if category == 'shower']\n",
    "    track_s_values = [track_s for track_l, track_s, category in track_likelihoods if category == 'track']\n",
    "    shower_s_values = [track_s for track_l, track_s, category in track_likelihoods if category == 'shower']\n",
    "\n",
    "\n",
    "    # Log-scaled Track_L\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(track_l_values, bins=50, color='red', alpha=0.7, label='Track (Track_L)', log=True)\n",
    "    plt.hist(shower_l_values, bins=50, color='blue', alpha=0.7, label='Shower (Track_L)', log=True)\n",
    "    plt.xlabel('Track_L')\n",
    "    plt.ylabel('Log Frequency')\n",
    "    plt.title('Log-Scaled Histogram of Track_L for Tracks and Showers')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Log-scaled Track_S\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(track_s_values, bins=50, color='red', alpha=0.7, label='Track (Track_S)', log=True)\n",
    "    plt.hist(shower_s_values, bins=50, color='blue', alpha=0.7, label='Shower (Track_S)', log=True)\n",
    "    plt.xlabel('Shower_L')\n",
    "    plt.ylabel('Log Frequency')\n",
    "    plt.title('Log-Scaled Histogram of Track_S for Tracks and Showers')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Main script to process events and plot\n",
    "likelihoods_log = []  # List to store (Track_L, Track_S, classification) for all events\n",
    "\n",
    "# Process each event\n",
    "for event_idx in range(len(events_unseen.reco_hits_w)):\n",
    "    result = likelihood_approach(events_unseen, event_idx, shower_pdg_set)\n",
    "    if result is not None:\n",
    "        likelihoods_log.append(result)\n",
    "\n",
    "# Plot the histograms\n",
    "plot_histograms_log(likelihoods_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7797f73b-ba78-477f-a538-453397980970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main script to process events and plot\n",
    "likelihoods2 = []  # List to store (Track_L, Track_S, classification) for all events\n",
    "num_tracks = 0  # Counter for tracks\n",
    "num_showers = 0  # Counter for showers\n",
    "\n",
    "# Process each event\n",
    "for event_idx in range(len(events_unseen.reco_hits_w)):\n",
    "    result = likelihood_approach(events_unseen, event_idx, shower_pdg_set)\n",
    "    if result is not None:\n",
    "        Track_L, Shower_L, classification = result\n",
    "        likelihoods2.append(result)\n",
    "        if classification == 'track':\n",
    "            num_tracks += 1\n",
    "        elif classification == 'shower':\n",
    "            num_showers += 1\n",
    "\n",
    "# Print the counts\n",
    "print(f\"Number of Tracks: {num_tracks}\")\n",
    "print(f\"Number of Showers: {num_showers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e1b9e4-a625-48f7-8f0c-23a47d5ecd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(events, shower_pdg_set):\n",
    "    \"\"\"Calculate the accuracy of the likelihood approach.\"\"\"\n",
    "    correct_predictions = 0\n",
    "    total_valid_events = 0  # Only count events that produce valid predictions\n",
    "\n",
    "    for event_idx in range(len(events_unseen.reco_hits_w)):\n",
    "        # Get ground truth classification\n",
    "        true_classification = categorise_event(events, event_idx, shower_pdg_set)\n",
    "\n",
    "        # Get likelihood values\n",
    "        result = likelihood_approach(events, event_idx, shower_pdg_set)\n",
    "        if result is not None:\n",
    "            Track_L, Shower_L, _ = result  # Ignore the true classification returned\n",
    "            predicted_classification = 'track' if Track_L > Shower_L else 'shower'\n",
    "\n",
    "            # Increment valid events count\n",
    "            total_valid_events += 1\n",
    "\n",
    "            # Compare predicted and true classification\n",
    "            if predicted_classification == true_classification:\n",
    "                correct_predictions += 1\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / total_valid_events if total_valid_events > 0 else 0\n",
    "    return accuracy, total_valid_events\n",
    "\n",
    "\n",
    "# Main script\n",
    "accuracy, total_valid_events = calculate_accuracy(events_unseen, shower_pdg_set)\n",
    "print(f\"Accuracy of the likelihood approach: {accuracy:.2%}\")\n",
    "print(f\"Total valid events processed: {total_valid_events}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7e247d-2ee0-4978-a8da-feb0b7dde3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def calculate_accuracy_and_metrics(events, shower_pdg_set):\n",
    "    \"\"\"Calculate accuracy and generate confusion matrix and classification report.\"\"\"\n",
    "    true_categories = []  # List to store ground truth classifications\n",
    "    predicted_categories = []  # List to store predicted classifications\n",
    "    total_valid_events = 0  # Only count events that produce valid predictions\n",
    "\n",
    "    for event_idx in range(len(events_unseen.reco_hits_w)):\n",
    "        # Get ground truth classification\n",
    "        true_classification = categorise_event(events, event_idx, shower_pdg_set)\n",
    "\n",
    "        # Get likelihood values\n",
    "        result = likelihood_approach(events, event_idx, shower_pdg_set)\n",
    "        if result is not None:\n",
    "            Track_L, Shower_L, _ = result  # Ignore the true classification returned\n",
    "            predicted_classification = 'track' if Track_L > Shower_L else 'shower'\n",
    "\n",
    "            # Append to lists for confusion matrix and classification report\n",
    "            true_categories.append(true_classification)\n",
    "            predicted_categories.append(predicted_classification)\n",
    "\n",
    "            # Increment valid events count\n",
    "            total_valid_events += 1\n",
    "\n",
    "    # Calculate accuracy\n",
    "    correct_predictions = sum(\n",
    "        1 for true, pred in zip(true_categories, predicted_categories) if true == pred\n",
    "    )\n",
    "    accuracy = correct_predictions / total_valid_events if total_valid_events > 0 else 0\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(true_categories, predicted_categories, labels=[\"shower\", \"track\"])\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Classification Report\n",
    "    report = classification_report(\n",
    "        true_categories, predicted_categories, target_names=[\"Shower\", \"Track\"], labels=[\"shower\", \"track\"]\n",
    "    )\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    return accuracy, total_valid_events, conf_matrix, report\n",
    "\n",
    "\n",
    "# Main script\n",
    "accuracy, total_valid_events, conf_matrix, report = calculate_accuracy_and_metrics(events_unseen, shower_pdg_set)\n",
    "print(f\"Accuracy of the likelihood approach: {accuracy:.2%}\")\n",
    "print(f\"Total valid events processed: {total_valid_events}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712a98b2-f19f-48d3-8c29-5c77eece8e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists for storing predicted and true categories\n",
    "true_categories = []  # Ground truth (Track vs. Shower)\n",
    "predicted_categories_noise = []\n",
    "predicted_categories_correlation = []\n",
    "predicted_categories_rms = []\n",
    "predicted_categories_angle = []\n",
    "predicted_categories_line = []\n",
    "predicted_categories_q4 = []\n",
    "\n",
    "\n",
    "\n",
    "# Iterate through events\n",
    "for event_idx in range(len(events_unseen.reco_hits_w)):\n",
    "    x_hits = events_unseen.reco_hits_x_w[event_idx]\n",
    "    w_hits = events_unseen.reco_hits_w[event_idx]\n",
    "\n",
    "    # Skip events with fewer than 15 hits\n",
    "    if len(w_hits) < 15:\n",
    "        continue\n",
    "\n",
    "    # Get predicted probabilities and classification results\n",
    "    correlation_score = correlation(events_unseen, event_idx)\n",
    "    noise_score = noise(events_unseen, event_idx)\n",
    "    rms_score = rms(events_unseen, event_idx)\n",
    "    angle_score = angle(events_unseen, event_idx)\n",
    "    line_score = line(events_unseen, event_idx)\n",
    "    q4_score = q4(events_unseen, event_idx)\n",
    "\n",
    "    # True label based on PDG code\n",
    "    true_label = categorise_event(events_unseen, event_idx, shower_pdg_set)\n",
    "    true_category = 1 if true_label == \"track\" else 0\n",
    "    true_categories.append(true_category)\n",
    "\n",
    "    # Get probabilities from histograms for Noise, Correlation, and RMS\n",
    "    if noise_score is not None:\n",
    "        prob_noise_track, prob_noise_shower = get_probabilities_from_histogram(\n",
    "            noise_score, noise_bin_edges, noise_track_counts, noise_shower_counts\n",
    "        )\n",
    "        predicted_category_noise = 1 if prob_noise_track > prob_noise_shower else 0  # Adjust threshold as needed\n",
    "        predicted_categories_noise.append(predicted_category_noise)\n",
    "    else:\n",
    "        predicted_categories_noise.append(0)  # If no score, classify as Shower by default\n",
    "\n",
    "    # Correlation classification\n",
    "    if correlation_score is not None:\n",
    "        prob_correlation_track, prob_correlation_shower = get_probabilities_from_histogram(\n",
    "            correlation_score, correlation_bin_edges, correlation_track_counts, correlation_shower_counts\n",
    "        )\n",
    "        predicted_category_correlation = 1 if prob_correlation_track > prob_correlation_shower else 0  # Adjust threshold as needed\n",
    "        predicted_categories_correlation.append(predicted_category_correlation)\n",
    "    else:\n",
    "        predicted_categories_correlation.append(0)  # If no score, classify as Shower by default\n",
    "\n",
    "    # RMS classification\n",
    "    if rms_score is not None:\n",
    "        prob_rms_track, prob_rms_shower = get_probabilities_from_histogram(\n",
    "            rms_score, rms_bin_edges, rms_track_counts, rms_shower_counts\n",
    "        )\n",
    "        predicted_category_rms = 1 if prob_rms_track > prob_rms_shower else 0  # Adjust threshold as needed\n",
    "        predicted_categories_rms.append(predicted_category_rms)\n",
    "    else:\n",
    "        predicted_categories_rms.append(0)  # If no score, classify as Shower by default\n",
    "\n",
    "\n",
    "    # angle classification\n",
    "    if angle_score is not None:\n",
    "        prob_angle_track, prob_angle_shower = get_probabilities_from_histogram(\n",
    "            angle_score, angles_bin_edges, angles_track_counts, angles_shower_counts\n",
    "        )\n",
    "        predicted_category_angle = 1 if prob_angle_track > prob_angle_shower else 0  # Adjust threshold as needed\n",
    "        predicted_categories_angle.append(predicted_category_angle)\n",
    "    else:\n",
    "        predicted_categories_angle.append(0)  # If no score, classify as Shower by default\n",
    "\n",
    "\n",
    "\n",
    "  # Line classification\n",
    "    if line_score is not None:\n",
    "        prob_line_track, prob_line_shower = get_probabilities_from_histogram(\n",
    "            line_score, line_bin_edges, line_track_counts, line_shower_counts\n",
    "        )\n",
    "        predicted_category_line = 1 if prob_line_track > prob_line_shower else 0  # Adjust threshold as needed\n",
    "        predicted_categories_line.append(predicted_category_line)\n",
    "    else:\n",
    "        predicted_categories_line.append(0)  # If no score, classify as Shower by default\n",
    "\n",
    "  # q4 classification\n",
    "    if q4_score is not None:\n",
    "        prob_q4_track, prob_q4_shower = get_probabilities_from_histogram(\n",
    "            q4_score, q4_bin_edges, q4_track_counts, q4_shower_counts\n",
    "        )\n",
    "        predicted_category_q4 = 1 if prob_q4_track > prob_q4_shower else 0  # Adjust threshold as needed\n",
    "        predicted_categories_q4.append(predicted_category_q4)\n",
    "    else:\n",
    "        predicted_categories_q4.append(0)  # If no score, classify as Shower by default\n",
    "\n",
    "# Ensure lengths match for true and predicted categories\n",
    "print(f\"Length of true_categories: {len(true_categories)}\")\n",
    "print(f\"Length of predicted_categories_noise: {len(predicted_categories_noise)}\")\n",
    "print(f\"Length of predicted_categories_correlation: {len(predicted_categories_correlation)}\")\n",
    "print(f\"Length of predicted_categories_rms: {len(predicted_categories_rms)}\")\n",
    "print(f\"Length of predicted_categories_angle: {len(predicted_categories_angle)}\")\n",
    "\n",
    "\n",
    "# Generate confusion matrices for each feature\n",
    "conf_matrix_noise = confusion_matrix(true_categories, predicted_categories_noise)\n",
    "conf_matrix_correlation = confusion_matrix(true_categories, predicted_categories_correlation)\n",
    "conf_matrix_rms = confusion_matrix(true_categories, predicted_categories_rms)\n",
    "conf_matrix_angle = confusion_matrix(true_categories, predicted_categories_angle)\n",
    "conf_matrix_line = confusion_matrix(true_categories, predicted_categories_line)\n",
    "conf_matrix_q4 = confusion_matrix(true_categories, predicted_categories_q4)\n",
    "\n",
    "# Print the confusion matrices\n",
    "print(\"Confusion Matrix (Noise):\")\n",
    "print(conf_matrix_noise)\n",
    "print(\"\\nConfusion Matrix (Correlation):\")\n",
    "print(conf_matrix_correlation)\n",
    "print(\"\\nConfusion Matrix (RMS):\")\n",
    "print(conf_matrix_rms)\n",
    "print(\"\\nConfusion Matrix (Angle):\")\n",
    "print(conf_matrix_angle)\n",
    "print(\"\\nConfusion Matrix (Line):\")\n",
    "print(conf_matrix_line)\n",
    "print(\"\\nConfusion Matrix (q4):\")\n",
    "print(conf_matrix_q4)\n",
    "\n",
    "\n",
    "# Plot the confusion matrices as heatmaps in a grid (2x3 layout)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))  # Create 6 subplots (2 rows, 3 columns)\n",
    "\n",
    "# Flatten axes for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot Noise Confusion Matrix\n",
    "sns.heatmap(conf_matrix_noise, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[\"Shower\", \"Track\"], yticklabels=[\"Shower\", \"Track\"], ax=axes[0])\n",
    "axes[0].set_title(\"Confusion Matrix (Noise)\")\n",
    "axes[0].set_xlabel(\"Predicted\")\n",
    "axes[0].set_ylabel(\"True\")\n",
    "\n",
    "# Plot Correlation Confusion Matrix\n",
    "sns.heatmap(conf_matrix_correlation, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[\"Shower\", \"Track\"], yticklabels=[\"Shower\", \"Track\"], ax=axes[1])\n",
    "axes[1].set_title(\"Confusion Matrix (Correlation)\")\n",
    "axes[1].set_xlabel(\"Predicted\")\n",
    "axes[1].set_ylabel(\"True\")\n",
    "\n",
    "# Plot RMS Confusion Matrix\n",
    "sns.heatmap(conf_matrix_rms, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[\"Shower\", \"Track\"], yticklabels=[\"Shower\", \"Track\"], ax=axes[2])\n",
    "axes[2].set_title(\"Confusion Matrix (RMS)\")\n",
    "axes[2].set_xlabel(\"Predicted\")\n",
    "axes[2].set_ylabel(\"True\")\n",
    "\n",
    "# Plot Angle Confusion Matrix\n",
    "sns.heatmap(conf_matrix_angle, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[\"Shower\", \"Track\"], yticklabels=[\"Shower\", \"Track\"], ax=axes[3])\n",
    "axes[3].set_title(\"Confusion Matrix (Angle)\")\n",
    "axes[3].set_xlabel(\"Predicted\")\n",
    "axes[3].set_ylabel(\"True\")\n",
    "\n",
    "# Plot Additional Confusion Matrix (e.g., Combined or Other Metric)\n",
    "sns.heatmap(conf_matrix_line, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[\"Shower\", \"Track\"], yticklabels=[\"Shower\", \"Track\"], ax=axes[4])\n",
    "axes[4].set_title(\"Confusion Matrix (line)\")\n",
    "axes[4].set_xlabel(\"Predicted\")\n",
    "axes[4].set_ylabel(\"True\")\n",
    "\n",
    "# Plot Additional Confusion Matrix (e.g., Combined or Other Metric)\n",
    "sns.heatmap(conf_matrix_q4, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[\"Shower\", \"Track\"], yticklabels=[\"Shower\", \"Track\"], ax=axes[4])\n",
    "axes[5].set_title(\"Confusion Matrix (q4)\")\n",
    "axes[5].set_xlabel(\"Predicted\")\n",
    "axes[5].set_ylabel(\"True\")\n",
    "\n",
    "\n",
    "plt.tight_layout()  # Adjust spacing to prevent overlap\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Optionally, display classification reports for each feature\n",
    "report_noise = classification_report(true_categories, predicted_categories_noise, target_names=[\"Shower\", \"Track\"])\n",
    "report_correlation = classification_report(true_categories, predicted_categories_correlation, target_names=[\"Shower\", \"Track\"])\n",
    "report_rms = classification_report(true_categories, predicted_categories_rms, target_names=[\"Shower\", \"Track\"])\n",
    "report_angle = classification_report(true_categories, predicted_categories_angle, target_names=[\"Shower\", \"Track\"])\n",
    "report_line = classification_report(true_categories, predicted_categories_line, target_names=[\"Shower\", \"Track\"])\n",
    "report_q4 = classification_report(true_categories, predicted_categories_q4, target_names=[\"Shower\", \"Track\"])\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report (Noise):\")\n",
    "print(report_noise)\n",
    "print(\"\\nClassification Report (Correlation):\")\n",
    "print(report_correlation)\n",
    "print(\"\\nClassification Report (RMS):\")\n",
    "print(report_rms)\n",
    "print(\"\\nClassification Report (Angle):\")\n",
    "print(report_angle)\n",
    "print(\"\\nClassification Report (line):\")\n",
    "print(report_line)\n",
    "print(\"\\nClassification Report (q4):\")\n",
    "print(report_q4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958d9b41-56f9-4fcd-abf4-d17fdd5044b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_event_hits_energy(x_hits, w_hits, adc_values, true_vertex_x, true_vertex_w, event_index):\n",
    "    plt.figure(figsize=(8, 6))  # Figure size\n",
    "    scatter = plt.scatter(w_hits, x_hits, c=adc_values, cmap='viridis', s=10, alpha=0.7)  # Scatter plot\n",
    "    plt.colorbar(scatter, label='ADC (Energy Deposit)')  # Generating Colour Bar\n",
    "    \n",
    "    # Plot the true neutrino interaction vertex\n",
    "    plt.scatter(true_vertex_w, true_vertex_x, c='red', marker='x', label=\"True Interaction Vertex\")\n",
    "    \n",
    "    # Labeling and plot settings\n",
    "    plt.title(f\"Event Display (W View) - Event {event_index}\")\n",
    "    plt.xlabel(\"W (Wire Position)\")\n",
    "    plt.ylabel(\"X (Drift Position)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def display_event(events, event_index):\n",
    "    # Initialize lists to collect all hits for the event\n",
    "    all_x_hits = []\n",
    "    all_w_hits = []\n",
    "    all_adc_values = []\n",
    "\n",
    "    # Loop over all sub-events for the specified event index\n",
    "    sub_events = np.where(events.event_number == events.event_number[event_index])[0]\n",
    "    for sub_event in sub_events:\n",
    "        x_hits = events.reco_hits_x_w[sub_event]  # Drift position (X)\n",
    "        w_hits = events.reco_hits_w[sub_event]     # Wire position (W)\n",
    "        adc_values = events.reco_adcs_w[sub_event] # Pulling energy values\n",
    "        \n",
    "        # Append the hits and ADC values to the lists\n",
    "        all_x_hits.extend(x_hits)\n",
    "        all_w_hits.extend(w_hits)\n",
    "        all_adc_values.extend(adc_values)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    all_x_hits = np.array(all_x_hits)\n",
    "    all_w_hits = np.array(all_w_hits)\n",
    "    all_adc_values = np.array(all_adc_values)\n",
    "\n",
    "    # True interaction vertex for the event\n",
    "    true_vertex_x = events.neutrino_vtx_x[event_index]\n",
    "    true_vertex_w = events.neutrino_vtx_w[event_index]\n",
    "\n",
    "    # Debugging prints to check data\n",
    "    print(f\"Event Index: {event_index}\")\n",
    "    print(f\"Total Hits Collected: {len(all_x_hits)}\")\n",
    "    if len(all_x_hits) == 0:\n",
    "        print(\"No hits found for this event.\")\n",
    "        return  # Exit if no hits\n",
    "\n",
    "    # Call the plotting function with extracted data\n",
    "    plot_event_hits_energy(all_x_hits, all_w_hits, all_adc_values, true_vertex_x, true_vertex_w, event_index)\n",
    "\n",
    "# Example usage:\n",
    "display_event(events_unseen, event_index=0)\n",
    "display_event(events_unseen, event_index=1)\n",
    "display_event(events_unseen, event_index=245)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be11fb8-4051-4929-814c-87198dd3aa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in true_categories:\", np.unique(true_categories))\n",
    "print(\"Unique values in predicted_categories:\", np.unique(predicted_categories))\n",
    "print(\"Lengths:\", len(true_categories), len(predicted_categories))\n",
    "\n",
    "\n",
    "# Ensure true_categories and predicted_categories are NumPy arrays\n",
    "true_categories = np.array(true_categories)\n",
    "predicted_categories = np.array(predicted_categories)\n",
    "\n",
    "false_positives = np.where((true_categories == 0) & (predicted_categories == 1))[0]\n",
    "false_negatives = np.where((true_categories == 1) & (predicted_categories == 0))[0]\n",
    "\n",
    "print(f\"Number of False Positives: {len(false_positives)}\")\n",
    "print(f\"Number of False Negatives: {len(false_negatives)}\")\n",
    "\n",
    "\n",
    "# Example: Display details for the first few false negatives\n",
    "for idx in false_negatives[:10]:  # Adjust the number as needed\n",
    "    print(f\"\\nEvent Index: {idx}\")\n",
    "    print(f\"True Category: {true_categories[idx]}\")\n",
    "    print(f\"Predicted Category: {predicted_categories[idx]}\")\n",
    "    print(f\"Track Probability: {track_probabilities[idx]}\")\n",
    "    print(f\"Shower Probability: {1 - track_probabilities[idx]}\")\n",
    "    # Optionally, display the event plot\n",
    "    display_event(events_unseen, event_index=idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09410dcf-d5e7-47f8-9b26-7038ef9c2d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC(t,s):\n",
    "    \"\"\"Plots ROC curve (Efficiency vs Purity) based on input PDFs of track and shower scores.\"\"\"\n",
    "    \n",
    "    eff = []  # Efficiency (True Positive Rate)\n",
    "    pur = []  # Purity (Precision)\n",
    "\n",
    "    # Iterate through threshold bins to calculate Efficiency and Purity\n",
    "    for i in range(len(t)):\n",
    "        shower_right = sum(s[i:])  # True Positives (TP)\n",
    "        both_right = sum(s[i:]) + sum(t[i:])  # Total Positive Predictions (TP + FP)\n",
    "        shower_all = sum(s)  # Total True Showers\n",
    "\n",
    "        # Calculate Efficiency\n",
    "        e = shower_right / shower_all if shower_all > 0 else 0\n",
    "\n",
    "        # Calculate Purity\n",
    "        if both_right != 0:\n",
    "            p = shower_right / both_right\n",
    "        else:\n",
    "            p = 1  # Avoid division by zero; purity is 1 if no predictions made\n",
    "\n",
    "        eff.append(e)\n",
    "        pur.append(p)\n",
    "\n",
    "    # Plot the ROC Curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(eff, pur, label=\"ROC Curve\", color='blue', linewidth=2)\n",
    "    plt.scatter(eff, pur, s=15, marker='x', color='red', label=\"ROC Points\")\n",
    "    plt.scatter(1, 1, c='orange', label=\"Ideal\", s=50, marker='o')\n",
    "    plt.xlabel(\"Efficiency (True Positive Rate)\")\n",
    "    plt.ylabel(\"Purity (Precision)\")\n",
    "    plt.title(\"Efficiency vs Purity Curve for Picking a Shower\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "ROC(track_probabilities, shower_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3239189-e587-4afa-883a-9d8330ec88f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list for storing false positives (Shower misclassified as Track)\n",
    "false_positive_shower_as_track = []\n",
    "\n",
    "# Iterate through events to identify false positives where true category is Shower, but classified as Track\n",
    "for event_idx in range(len(events_unseen.reco_hits_w)):\n",
    "    # Get the scores for individual features\n",
    "    correlation_score = correlation(events_unseen, event_idx)\n",
    "    noise_score = noise(events_unseen, event_idx)\n",
    "    rms_score = rms(events_unseen, event_idx)\n",
    "\n",
    "    # True label based on PDG code\n",
    "    true_label = categorise_event(events_unseen, event_idx, shower_pdg_set)\n",
    "    true_category = 1 if true_label == \"track\" else 0  # 1 = Track, 0 = Shower\n",
    "\n",
    "    # Skip events where any score is None\n",
    "    if correlation_score is None or noise_score is None or rms_score is None:\n",
    "        continue\n",
    "\n",
    "    # Get probabilities from histograms\n",
    "    prob_correlation_track, prob_correlation_shower = get_probabilities_from_histogram(\n",
    "        correlation_score, correlation_bin_edges, correlation_track_counts, correlation_shower_counts\n",
    "    )\n",
    "\n",
    "    prob_noise_track, prob_noise_shower = get_probabilities_from_histogram(\n",
    "        noise_score, noise_bin_edges, noise_track_counts, noise_shower_counts\n",
    "    )\n",
    "\n",
    "    prob_rms_track, prob_rms_shower = get_probabilities_from_histogram(\n",
    "        rms_score, rms_bin_edges, rms_track_counts, rms_shower_counts\n",
    "    )\n",
    "\n",
    "    # Compute combined likelihood (product of individual likelihoods)\n",
    "    likelihood_track = prob_correlation_track * prob_noise_track * prob_rms_track\n",
    "    likelihood_shower = prob_correlation_shower * prob_noise_shower * prob_rms_shower\n",
    "\n",
    "    # Final classification based on combined likelihood\n",
    "    predicted_category = 1 if likelihood_track > likelihood_shower else 0  # 1 = Track, 0 = Shower\n",
    "\n",
    "    # Identify false positives where true category is Shower (0) but predicted as Track (1)\n",
    "    if true_category == 0 and predicted_category == 1:\n",
    "        false_positive_shower_as_track.append(event_idx)\n",
    "\n",
    "# Take a subset of false positives (e.g., first 10 for visualization)\n",
    "sample_false_positives_shower_as_track = false_positive_shower_as_track[:10]\n",
    "\n",
    "# Function to plot event display\n",
    "def plot_event_display(event_idx, events_unseen):\n",
    "    # Extract x, w, and other features for the event\n",
    "    x_hits = events_unseen.reco_hits_x_w[event_idx]\n",
    "    w_hits = events_unseen.reco_hits_w[event_idx]\n",
    "    \n",
    "    # Create a scatter plot for the hits (simple visualization)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(x_hits, w_hits, c='blue', label=\"Hits\")\n",
    "    plt.title(f\"Event {event_idx} Display\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"W\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ed2309-4d95-48aa-91ff-22c077433d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 10 false positives (Shower misclassified as Track)\n",
    "for event_idx in sample_false_positives_shower_as_track:\n",
    "    plot_event_display(event_idx, events_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbb3951-d809-439d-98aa-b454ca373c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume we have these lists from the data\n",
    "true_labels = []  # 1 for \"Track\", 0 for \"Shower\"\n",
    "predicted_probs = []  # Corresponding Track_L values\n",
    "\n",
    "# Example loop to collect the data\n",
    "for event_idx in range(len(events_unseen.reco_hits_w)):\n",
    "    result = likelihood_approach(events_unseen, event_idx)\n",
    "    if result is not None:\n",
    "        classification, likelihood, *_ = result\n",
    "        # Get true category based on PDG code\n",
    "        true_label = categorise_event(events_unseen, event_idx, shower_pdg_set)\n",
    "        true_category = 1 if true_label == \"track\" else 0\n",
    "        predicted_probs.append(likelihood)\n",
    "        true_labels.append(true_category)\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(true_labels, predicted_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db3ff15-0be6-45d3-be10-c0421cd309f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Placeholder lists for true labels and predicted probabilities\n",
    "true_labels = []  # 1 for \"Track\", 0 for \"Shower\"\n",
    "predicted_probs = []  # Corresponding Track_L values\n",
    "\n",
    "# Example loop to collect the data\n",
    "for event_idx in range(len(events_unseen.reco_hits_w)):\n",
    "    result = likelihood_approach(events_unseen, event_idx)\n",
    "    if result is not None:\n",
    "        classification, likelihood, *_ = result\n",
    "        true_label = categorise_event(events_unseen, event_idx, shower_pdg_set)\n",
    "        true_category = 1 if true_label == \"track\" else 0\n",
    "        predicted_probs.append(likelihood)\n",
    "        true_labels.append(true_category)\n",
    "\n",
    "# Convert to numpy arrays for easier processing\n",
    "true_labels = np.array(true_labels)\n",
    "predicted_probs = np.array(predicted_probs)\n",
    "\n",
    "# Arrays to store efficiency and purity\n",
    "efficiencies = []\n",
    "purities = []\n",
    "\n",
    "# Thresholds from 0 to 1\n",
    "thresholds = np.linspace(0, 1, 100)\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Predicted labels based on threshold\n",
    "    predicted_labels = (predicted_probs > threshold).astype(int)\n",
    "    \n",
    "    # True Positives, False Positives, False Negatives\n",
    "    TP = np.sum((predicted_labels == 1) & (true_labels == 1))\n",
    "    FP = np.sum((predicted_labels == 1) & (true_labels == 0))\n",
    "    FN = np.sum((predicted_labels == 0) & (true_labels == 1))\n",
    "    \n",
    "    # Compute efficiency and purity\n",
    "    efficiency = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    purity = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    \n",
    "    efficiencies.append(efficiency)\n",
    "    purities.append(purity)\n",
    "\n",
    "# Plot efficiency vs purity\n",
    "plt.figure()\n",
    "plt.plot(efficiencies, purities, marker='o', color='blue', label='Efficiency vs Purity')\n",
    "plt.xlabel('Efficiency (Recall)')\n",
    "plt.ylabel('Purity (Precision)')\n",
    "plt.title('Efficiency vs Purity')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e5dc11-ac2c-44a4-89e5-be9c7bce094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def ROC(track_probabilities, shower_probabilities):\n",
    "    \"\"\"\n",
    "    Plots ROC curve (Efficiency vs Purity) based on input PDFs of track and shower scores.\n",
    "    \n",
    "    Parameters:\n",
    "    - track_probabilities: list or array of probabilities for track events\n",
    "    - shower_probabilities: list or array of probabilities for shower events\n",
    "    \"\"\"\n",
    "    eff = []  # Efficiency (True Positive Rate)\n",
    "    pur = []  # Purity (Precision)\n",
    "\n",
    "    # Ensure both probabilities are sorted\n",
    "    track_probabilities = sorted(track_probabilities)\n",
    "    shower_probabilities = sorted(shower_probabilities)\n",
    "\n",
    "    # Iterate through threshold bins\n",
    "    for i in range(len(track_probabilities)):\n",
    "        # True Positives (Shower events correctly identified)\n",
    "        shower_right = sum(shower_probabilities[i:])  \n",
    "        \n",
    "        # Total Positive Predictions (Shower + Track above threshold)\n",
    "        both_right = sum(shower_probabilities[i:]) + sum(track_probabilities[i:])  \n",
    "\n",
    "        # Total True Showers\n",
    "        shower_all = sum(shower_probabilities)\n",
    "\n",
    "        # Efficiency (True Positive Rate)\n",
    "        e = shower_right / shower_all if shower_all > 0 else 0\n",
    "\n",
    "        # Purity (Precision)\n",
    "        p = shower_right / both_right if both_right != 0 else 1  # Avoid division by zero\n",
    "\n",
    "        eff.append(e)\n",
    "        pur.append(p)\n",
    "\n",
    "    # Plot the ROC Curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(eff, pur, label=\"ROC Curve\", color='blue', linewidth=2)\n",
    "    plt.scatter(eff, pur, s=15, marker='x', color='red', label=\"ROC Points\")\n",
    "    plt.scatter(1, 1, c='orange', label=\"Ideal\", s=50, marker='o')\n",
    "    plt.xlabel(\"Efficiency (True Positive Rate)\")\n",
    "    plt.ylabel(\"Purity (Precision)\")\n",
    "    plt.title(\"Efficiency vs Purity Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "ROC(track_probabilities, shower_probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5143bd1b-376e-4f76-a84c-3f1c74b48335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Iterate through events to compute probabilities\n",
    "for event_idx in range(len(events_unseen.reco_hits_w)):\n",
    "    result = likelihood_approach(events_unseen, event_idx)\n",
    "    \n",
    "    if result is not None:\n",
    "        # Unpack the returned result\n",
    "        classification, likelihood, prob_correlation, prob_noise, prob_rms, prob_angle, prob_line, prob_q4 = result\n",
    "\n",
    "        # Store the likelihood based on classification\n",
    "        if classification == \"Track\":\n",
    "            track_probabilities.append(likelihood)\n",
    "        elif classification == \"Shower\":\n",
    "            shower_probabilities.append(1 - likelihood)  # Flip likelihood for Shower\n",
    "        \n",
    "        # Store the individual feature probabilities for analysis\n",
    "        correlation_probs.append(prob_correlation)\n",
    "        noise_probs.append(prob_noise)\n",
    "        rms_probs.append(prob_rms)\n",
    "        angle_probs.append(prob_angle)\n",
    "        line_probs.append(prob_line)\n",
    "        q4_probs.append(prob_q4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create correlation matrix\n",
    "matrix = np.corrcoef(correlation_probs, noise_probs, rms_probs, angle_probs, line_probs, q4_probs)\n",
    "print(matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
